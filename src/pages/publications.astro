---
import BaseLayout from "@layouts/BaseLayout.astro";
import HorizontalCardPublication from "@components/HorizontalCardPublication.astro";
---

<BaseLayout title="Publications" sideBarActiveItemID="publications">
    
    <div>
        <div class="text-3xl w-full font-bold mb-5">Publications</div>
        You can find my publication on <a class="underline" href="https://scholar.google.com/citations?user=X1_qLV4AAAAJ&hl=it">Google Scholar</a> or <a class="underline" href="https://www.semanticscholar.org/author/Michele-Papucci/3757153">Semantic Scholar</a>. However here's the complete publication list curated by me:
    </div>
    <HorizontalCardPublication
        title="Generating and Evaluating Multi-Level Text Simplification: A Case Study on Italian"
        authors="Michele Papucci, Giulia Venturi, Felice Dell'Orletta"
        published_at="Accepted at Proceedings of the Eleventh Italian Conference on Computational Linguistics (CLiC-it 2025), Cagliari."
        abstract="Recent advances in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, yet controlling model outputs remains a challenge. In this study, we explore the use of LLMs to generate high-quality synthetic data for Automatic Text Simplification (ATS), evaluating the ability of models fine-tuned on Italian to produce multiple simplified versions of the same original sentence that vary in readability and in their lexical and (morpho-)syntactic characteristics. The approach is tested across two domains, Wikipedia and Public Administration, allowing us to explore domain sensitivity. Additionally, we compare the linguistic phenomena observed in the generated data with those found in ATS resources previously created through manual or semi-automatic methods. Our results suggest that the best-performing LLM can generate linguistically diverse simplifications that align with known simplification patterns, offering a promising direction for building reliable ATS resources, including simplifications suited to varying levels of reader proficiency."
        badge="Proceedings"
        year="2025"
        bibtex={`Coming Soon`}
        bibtex_id = multi_level_semplification
        code_url = "https://github.com/michelepapucci/multilevel-text-simplification-italian"
        code_text = "Resources Available"
        tags={["Text Simplification", "Controllable Text Generation"]}
    />
    <HorizontalCardPublication
        title="Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors"
        authors="Andrea Pedrotti, Michele Papucci, Cristiano Ciaccio, Alessio Miaschi, Giovanni Puccetti, Felice Dell'Orletta, Andrea Esuli"
        published_at="Findings of the Association for Computational Linguistics: ACL 2025, Vienna, Austria."
        abstract="Recent advancements in Generative AI and Large Language Models (LLMs) have enabled the creation of highly realistic synthetic content, raising concerns about the potential for malicious use, such as misinformation and manipulation. Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the lack of robust benchmarks that assess generalization to real-world scenarios. In this work, we present a pipeline to test the resilience of state-of-the-art MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed adversarial attacks. To challenge the detectors, we fine-tune language models using Direct Preference Optimization (DPO) to shift the MGT style toward human-written text (HWT). This exploits the detectors’ reliance on stylistic clues, making new generations more challenging to detect. Additionally, we analyze the linguistic shifts induced by the alignment and which features are used by detectors to de tect MGT texts. Our results show that detectors can be easily fooled with relatively few examples, resulting in a significant drop in detection performance. This highlights the importance of improving detection methods and making them robust to unseen in-domain texts. We release code, models, and data to support fu ture research on more robust MGT detection benchmarks."
        badge="Proceedings"
        year="2025"
        bibtex={`@inproceedings{pedrotti-etal-2025-stress,
                title = "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors",
                author = "Pedrotti, Andrea  and
                Papucci, Michele  and
                Ciaccio, Cristiano  and
                Miaschi, Alessio  and
                Puccetti, Giovanni  and
                Dell{'}Orletta, Felice  and
                Esuli, Andrea",
                editor = "Che, Wanxiang  and
                Nabende, Joyce  and
                Shutova, Ekaterina  and
                Pilehvar, Mohammad Taher",
                booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
                month = jul,
                year = "2025",
                address = "Vienna, Austria",
                publisher = "Association for Computational Linguistics",
                url = "https://aclanthology.org/2025.findings-acl.156/",
                pages = "3010--3031",
                ISBN = "979-8-89176-256-5"
            }`}
        bibtex_id = pedrotti-etal-2025-stress
        url="https://aclanthology.org/2025.findings-acl.156/"
        code_url="https://github.com/gpucce/control_mgt"
        tags={["Machine Generated Text Detection", "Controllable Text Generation"]}
    />
    <HorizontalCardPublication
        title="Fantastic Labels and Where to Find Them: Attention-Based Label Selection for Text-to-Text Classification"
        authors="Michele Papucci, Alessio Miaschi, Felice Dell'Orletta"
        published_at="Proceedings of the 8th Work-shop on Natural Language for Artificial Intelligence (NL4AI 2024) co-located with 23th International Conference of the Italian Association for Artificial Intelligence (AI*IA 2024), Bolzano."
        abstract="Generative language models, particularly adopting text-to-text frameworks, have shown significant success in NLP tasks. While much research has focused on input representations via prompting techniques, less attention has been given to optimizing output representations. Previous studies found inconsistent effects of label representations on model performance in classification tasks using these models. In this work, we introduce a novel method for selecting well-performing label representations by leveraging the attention mechanisms of Transformer models. We used an Italian T5 model fine-tuned on a topic classification task, trained on posts extracted from online forums and categorized into 11 classes, to evaluate different label representation selection strategies. We’ve employed a context-mixing score called Value Zeroing to assess each token’s impact to select possible representations from the training set. Our results include a detailed qualitative analysis to identify which label choices most significantly affect classification outcomes, suggesting that using our approach to select label representations can enhance performance."
        badge="Proceedings"
        year="2024"
        bibtex_id = fantastic_labels
        url="/pdfs/fantastic_labels.pdf"
        code_url="https://github.com/michelepapucci/fantastic-labels"
        bibtex={`@inproceedings{
            papucci2024fantastic,
            title={Fantastic Labels and Where to Find Them: Attention-Based Label Selection for Text-to-Text Classification.},
            author={Papucci, Michele and Miaschi, Alessio and Dell'Orletta, Felice},
            booktitle={NL4AI@ AI* IA},
            year={2024}
          }`}
        tags={["Label Representation"]}
    />
    <HorizontalCardPublication
        title="Label Selection in Text-to-Text Neural Language Models for Classification"
        authors="Michele Papucci"
        published_at="University of Pisa, Master's Thesis in Data Science and Business Informatics"
        badge="Thesis"
        year="2024"
        abstract="This work contains a set of preliminary experiments with the objective of exploring and optimizing the use of reasonably small text-to-text Transformers to solve classification tasks. The broader objective is to see if we can use text-to-text Language Models, that aren’t costly to train and deploy like the ones that are currently very popular (e.g. Chat-GPT or LLaMa), as a unifying framework to solve any Natural Language Processing tasks. Contrary to what we need to do with larger models, with reasonably sized Transformers we need to find optimal way of casting the tasks into a text-to-text form, i.e. having a textual input, and expecting a textual output from the model. This thesis focuses on classification tasks, and in particular on the problem of how to represent class names into the best possible strings that maximize performances for the model. First, we evaluated whether this smaller models can obtain reasonable performances in classification tasks. Then, we tested the importance of label representation in this settings, finding that is, indeed, important to maximize the model performances. Finally, we presented and evaluated a novel technique to extract label representation from the training set of a classification task based on Attention attribution explainability methods."
        url="/pdfs/tesi_magistrale.pdf"
        bibtex={`@mastersthesis{
            papucci2024thesis,
            title={Label Selection in Text-to-Text Neural Language Models for Classification},
            author={Papucci, Michele},
            school={University of Pisa},
            year={2024}
          }`}
        bibtex_id = master_thesis
        tags={["Label Representation"]}
    />
    <HorizontalCardPublication
        title="Lost in Labels: An Ongoing Quest to Optimize Text-to-Text Label Selection for Classification"
        authors="Michele Papucci, Alessio Miaschi, Felice Dell'Orletta"
        published_at="Proceedings of the Ninth Italian Conference on Computational Linguistics (CLiC-it 2023), Venezia."
        abstract="In this paper, we present an evaluation of the influence of label selection on the performance of a Sequence-to-Sequence Transformer model in a classification task. Our study investigates whether the choice of words used to represent classification categories affects the model’s performance, and if there exists a relationship between the model’s performance and the selected words. To achieve this, we fine-tuned an Italian T5 model on topic classification using various labels. Our results indicate that the different label choices can significantly impact the model’s performance. That being said, we did not find a clear answer on how these choices affect the model performances, highlighting the need for further research in optimizing label selection."
        badge="Proceedings"
        year="2023"
        url="https://ceur-ws.org/Vol-3596/paper39.pdf"
        bibtex_id="lost_in_labels"
        bibtex={`@inproceedings{
                papucci2024lost,
                title={Lost in Labels: An Ongoing Quest to Optimize Text-to-Text Label Selection for Classification},
                author={Papucci, Michele and Miaschi, Alessio and Dell’Orletta, Felice},
                booktitle={Proceedings of the 9th Italian Conference on Computational Linguistics CLiC-it 2023: Venice, Italy, November 30-December 2, 2023},
                volume={516},
                number={394},
                pages={343},
                year={2024},
                organization={Accademia University Press}
                }`}
        tags={["Label Representation"]}
    />
    <HorizontalCardPublication
        title="Evaluating Text-To-Text Framework for Topic and Style Classification of Italian texts"
        authors="Michele Papucci, Chiara De Nigris, Alessio Miaschi, Felice Dell'Orletta"
        published_at="Proceedings of the 6th Work-shop on Natural Language for Artificial Intelligence (NL4AI 2022) co-located with 21th International Conference of the Italian Association for Artificial Intelligence (AI*IA 2022), Udine"
        abstract="In this paper, we propose an extensive evaluation of the first text-to-text Italian Neural Language Model (NLM), IT5, on a classification scenario. In particular, we test the performance of IT5 on several tasks involving both the classification of the topic and the style of a set of Italian posts. We assess the model in two different configurations, single-and multi-task classification, and we compare it with a more traditional NLM based on the Transformer architecture (i.e. BERT). Moreover, we test its performance in a few-shot learning scenario. We also perform a qualitative investigation on the impact of label representations in modeling the classification of the IT5 model. Results show that IT5 could achieve good results, although generally lower than the BERT model. Nevertheless, we observe a significant performance improvement of the Text-to-text model in a multi-task classification scenario. Finally, we found that altering the representation of the labels mainly impacts the classification of the topic."
        badge="Proceedings"
        year="2022"
        url="http://sag.art.uniroma2.it/NL4AI/wp-content/uploads/2022/11/paper8.pdf"
        bibtex={`@inproceedings{
            papucci2022evaluating,
            title={Evaluating Text-To-Text Framework for Topic and Style Classification of Italian texts.},
            author={Papucci, Michele and De Nigris, Chiara and Miaschi, Alessio and Dell'Orletta, Felice},
            booktitle={NL4AI@ AI* IA},
            pages={56--70},
            year={2022}
          }`}
        bibtex_id="evaluating_text_to_text"
        tags={["Label Representation"]}
    />
    <HorizontalCardPublication
        title="L'Italiano Amministrativo dagli Atti al Web: una risorsa per la semplificazione automatica"
        authors="Michele Papucci"
        published_at="University of Pisa, Bachelor's Thesis in Informatica Umanistica (Digital Humanities)"
        badge="Thesis"
        year="2022"
        url="/pdfs/tesi_triennale.pdf"
        language="Italian"
        bibtex={`@mastersthesis{
            papucci2022thesis,
            title={L'Italiano Amministrativo dagli Atti al Web: una risorsa per la semplificazione automatica},
            author={Papucci, Michele},
            school={University of Pisa},
            year={2022}
            type="Bachelor Thesis"
          }`}
        bibtex_id = bachelor_thesis
        tags={["Text Simplification"]}
    />
    <div class="divider my-0" />
</BaseLayout>
